{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e894ce36-f0ce-46ad-9566-1987a2c82534",
   "metadata": {},
   "source": [
    "# 3. LangChain基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3f4578-51a1-48fe-85ce-aae5c03bcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4cf15-7f64-42dd-80b3-88c9842fd55e",
   "metadata": {},
   "source": [
    "## 3.4. LangChain的回调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a595a2-84be-4928-9df8-5cabb6cfbd92",
   "metadata": {},
   "source": [
    "### 3.4.2. 认识异步回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85fbe96-60a0-4a2c-8fb5-49fc7813973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zzzz....\n",
      "LLM正在启动\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 烤\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 鸭\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 、\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 炸\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 酱\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 面\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 、\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 豆\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 汁\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 、\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 驴\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 打\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 滚\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 、\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 卤\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 煮\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 、\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 糖\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 葫\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 芦\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: 。\n",
      "正在 thread_pool_executor 中调用同步处理程序: token: \n",
      "zzzz....\n",
      "LLM结束\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='烤鸭、炸酱面、豆汁、驴打滚、卤煮、糖葫芦。', generation_info={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:52:17.135373423Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 1961936666, 'load_duration': 123305565, 'prompt_eval_count': 379, 'prompt_eval_duration': 9932778, 'eval_count': 29, 'eval_duration': 140713750}, message=AIMessage(content='烤鸭、炸酱面、豆汁、驴打滚、卤煮、糖葫芦。', response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:52:17.135373423Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 1961936666, 'load_duration': 123305565, 'prompt_eval_count': 379, 'prompt_eval_duration': 9932778, 'eval_count': 29, 'eval_duration': 140713750}, id='run-933bd4de-2b2c-49cc-aa44-3058a5b3f4c9-0'))]], llm_output={}, run=[RunInfo(run_id=UUID('933bd4de-2b2c-49cc-aa44-3058a5b3f4c9'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"正在 thread_pool_executor 中调用同步处理程序: token: {token}\")\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"可用于处理来自LangChain的回调异步回调处理程序。\"\"\"\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
    "        \"\"\"在链开始时运行。\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        print(\"LLM正在启动\")\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"在链结束时运行。\"\"\"\n",
    "        print(\"zzzz....\")\n",
    "        await asyncio.sleep(0.3)\n",
    "        print(\"LLM结束\")\n",
    "\n",
    "\n",
    "# 为了启用流式传输，在ChatModel()构造函数中传入streaming=True\n",
    "# 此外，还传入了一个包含自定义处理程序的列表\n",
    "\n",
    "\"\"\"\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:11434/v1\"\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key = openai_api_key, \n",
    "    openai_api_base = openai_api_base,\n",
    "    temperature=0, \n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],)\n",
    "\"\"\"\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0, \n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],)\n",
    "\n",
    "await chat.agenerate([[HumanMessage(content=\"告诉我北京的特产，回答不要超过32个字\")]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26f35a-fd34-4c3b-8f97-43b7ffacf866",
   "metadata": {},
   "source": [
    "### 3.4.4. 自定义回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e69cc7-e35e-46c9-b43a-1c20199c3191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GithubProjects/HelloAI/Book/LangChain实战派/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: \n",
      "自定义回调处理器, token: 中国\n",
      "自定义回调处理器, token: 的\n",
      "自定义回调处理器, token: 首\n",
      "自定义回调处理器, token: 都\n",
      "自定义回调处理器, token: 就是\n",
      "自定义回调处理器, token: **\n",
      "自定义回调处理器, token: 北京\n",
      "自定义回调处理器, token: **\n",
      "自定义回调处理器, token: 。\n",
      "自定义回调处理器, token: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='中国的首都就是**北京**。', response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:52:27.741146539Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 486122284, 'load_duration': 120562873, 'prompt_eval_count': 116, 'prompt_eval_duration': 9564759, 'eval_count': 10, 'eval_duration': 43851537}, id='run-f8c23878-a1ce-4844-bf22-360677da7493-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"自定义回调处理器, token: {token}\")\n",
    "\n",
    "\"\"\"\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:11434/v1\"\n",
    "chat = ChatOpenAI(max_tokens=25, streaming=True,openai_api_key = openai_api_key,openai_api_base = openai_api_base, callbacks=[MyCustomHandler()])\n",
    "\"\"\"\n",
    "\n",
    "chat = ChatOllama(model=\"gpt-oss:20b\", max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])\n",
    "chat([HumanMessage(content=\"中国首都是？\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0fe0-23dd-4866-8fc6-46bfa3a7d99d",
   "metadata": {},
   "source": [
    "### 3.4.5. 使用回调记录日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cc5664-3473-4e13-b570-02e6bfec0ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GithubProjects/HelloAI/Book/LangChain实战派/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 1 = \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 00:52:45.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1m2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "\"\"\"\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:11434/v1\"\n",
    "llm = OpenAI(openai_api_key = openai_api_key, openai_api_base = openai_api_base,)\n",
    "\"\"\"\n",
    "llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# 这个链将同时向标准输出打印（因为 verbose=True）并写入 'output.log'\n",
    "# 如果 verbose=False，FileCallbackHandler 仍会写入 'output.log'\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)\n",
    "answer = chain.run(number=1)\n",
    "logger.info(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690784cf-98b5-4db2-a981-ec56107ff1e1",
   "metadata": {},
   "source": [
    "### 3.4.6. 处理多个回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33565295-1505-40d5-861a-c330492e9472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GithubProjects/HelloAI/Book/LangChain实战派/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start AgentExecutor\n",
      "on_chain_start LLMChain\n",
      "on_llm_start Ollama\n",
      "on_llm_start (I'm the second handler!!) Ollama\n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token \n",
      "on_new_token 影\n",
      "on_new_token 子\n",
      "on_new_token 之间\n",
      "on_new_token 存在\n",
      "on_new_token 吸\n",
      "on_new_token 力\n",
      "on_new_token 是\n",
      "on_new_token 由于\n",
      "on_new_token 光\n",
      "on_new_token 在\n",
      "on_new_token 空间\n",
      "on_new_token 中\n",
      "on_new_token 沿着\n",
      "on_new_token 直线\n",
      "on_new_token 传播\n",
      "on_new_token ，\n",
      "on_new_token 当\n",
      "on_new_token 遮\n",
      "on_new_token 挡\n",
      "on_new_token 部分\n",
      "on_new_token 周围\n",
      "on_new_token 没有\n",
      "on_new_token 光\n",
      "on_new_token 时\n",
      "on_new_token ，\n",
      "on_new_token 会\n",
      "on_new_token 形成\n",
      "on_new_token 阴影\n",
      "on_new_token 。\n",
      "on_new_token 由于\n",
      "on_new_token 光线\n",
      "on_new_token 的\n",
      "on_new_token 路径\n",
      "on_new_token 、\n",
      "on_new_token 角度\n",
      "on_new_token 以及\n",
      "on_new_token 障碍\n",
      "on_new_token 物\n",
      "on_new_token 的位置\n",
      "on_new_token 不同\n",
      "on_new_token ，\n",
      "on_new_token 这些\n",
      "on_new_token 区域\n",
      "on_new_token 会在\n",
      "on_new_token 空间\n",
      "on_new_token 中\n",
      "on_new_token 向\n",
      "on_new_token 彼此\n",
      "on_new_token 靠近\n",
      "on_new_token 或\n",
      "on_new_token 远离\n",
      "on_new_token ，\n",
      "on_new_token 从而\n",
      "on_new_token 表现出\n",
      "on_new_token 引力\n",
      "on_new_token 的作用\n",
      "on_new_token 。\n",
      "\n",
      "**\n",
      "on_new_token Final\n",
      "on_new_token  Answer\n",
      "on_new_token :**\n",
      "on_new_token  影\n",
      "on_new_token 子\n",
      "on_new_token 之间\n",
      "on_new_token 有\n",
      "on_new_token 吸\n",
      "on_new_token 力\n",
      "on_new_token 是因为\n",
      "on_new_token 光\n",
      "on_new_token 沿\n",
      "on_new_token 直线\n",
      "on_new_token 传播\n",
      "on_new_token ，\n",
      "on_new_token 遮\n",
      "on_new_token 挡\n",
      "on_new_token 后\n",
      "on_new_token 形成的\n",
      "on_new_token 阴影\n",
      "on_new_token 在\n",
      "on_new_token 不同\n",
      "on_new_token 位置\n",
      "on_new_token 因为\n",
      "on_new_token 光线\n",
      "on_new_token 路径\n",
      "on_new_token 的不同\n",
      "on_new_token 而\n",
      "on_new_token 逐渐\n",
      "on_new_token 靠近\n",
      "on_new_token 或\n",
      "on_new_token 分开\n",
      "on_new_token ，\n",
      "on_new_token 这\n",
      "on_new_token 与\n",
      "on_new_token 光\n",
      "on_new_token 的\n",
      "on_new_token 直线\n",
      "on_new_token 传播\n",
      "on_new_token 和\n",
      "on_new_token 障碍\n",
      "on_new_token 物\n",
      "on_new_token 的影响\n",
      "on_new_token 有关\n",
      "on_new_token 。\n",
      "on_new_token \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'** 影子之间有吸力是因为光沿直线传播，遮挡后形成的阴影在不同位置因为光线路径的不同而逐渐靠近或分开，这与光的直线传播和障碍物的影响有关。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 定义自定义回调处理器\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n",
    "\n",
    "# 实例化处理器\n",
    "handler1 = MyCustomHandlerOne()\n",
    "handler2 = MyCustomHandlerTwo()\n",
    "\n",
    "# 设置代理。只有“llm”会为handler2发出回调\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\", callbacks=[handler2])\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "# handler1的回调将由参与Agent执行的每个对象（llm、llmchain、tool、agent executor）发出\n",
    "agent.run(\"为什么影子之间会有吸力?\", callbacks=[handler1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a43f2-da3a-4c68-8d05-e838db3ed9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
