{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205b6ef7-ff7f-48e6-adf5-9af77dc2f5dd",
   "metadata": {},
   "source": [
    "# 3. LangChainåŸºç¡€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d3ba46-c4fd-46dc-81ec-efc1e065d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d19abb-1255-4614-a39b-18672d37696f",
   "metadata": {},
   "source": [
    "## 3.3. LangChainçš„å›é€€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793a546-af67-4a39-9ec1-dda3414f0834",
   "metadata": {},
   "source": [
    "### 3.3.1. å¤„ç†LLMè°ƒç”¨APIçš„é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6288b82e-b618-4e6b-b157-3a758476b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª RateLimitError å¯¹è±¡\n",
    "response = httpx.Response(\n",
    "    status_code=429,                                                       # 429: Too Many Requests é”™è¯¯ç \n",
    "    request=httpx.Request('GET', 'https://api.openai.com/v1/completions')  # æ¨¡æ‹Ÿä¸€ä¸ªè¯·æ±‚å¯¹è±¡\n",
    ")\n",
    "\n",
    "body = {\n",
    "    'error': {\n",
    "        'message': 'Rate limit exceeded.',\n",
    "        'type': 'rate_limit_error',\n",
    "        'param': None,\n",
    "        'code': 'rate_limit_error'\n",
    "    }\n",
    "}\n",
    "\n",
    "error = RateLimitError(message=\"Rate limit exceeded.\", response=response, body=body)\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40e542-7a1f-45b5-982a-abb6d17a17f6",
   "metadata": {},
   "source": [
    "#### 1. æ¨¡æ‹Ÿè§¦å‘é€Ÿç‡é™åˆ¶é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26c96ec-a899-46bf-b3d8-21efd0b588bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡åˆ°é”™è¯¯\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "openai_llm = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, temperature=0, max_tokens=256, max_retries=0)\n",
    "\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(openai_llm.invoke(\"ä½ æ˜¯è°ï¼Ÿ\"))\n",
    "    except RateLimitError:\n",
    "        print(\"é‡åˆ°é”™è¯¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a2cb6-5456-4f7a-b2e8-25bbe9b67a22",
   "metadata": {},
   "source": [
    "#### 2. æµ‹è¯•å›é€€åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e274944-b438-4a10-b999-5971e25aea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='æˆ‘æ˜¯ä¸€åç”± OpenAI è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåå­—å« ChatGPTã€‚æˆ‘çš„å·¥ä½œæ˜¯å¸®åŠ©ä½ å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©å†™ä½œã€ç¿»è¯‘ã€è®¨è®ºå„ç§è¯é¢˜ç­‰ã€‚ä½ æœ‰ä»€ä¹ˆæƒ³èŠçš„ï¼Œæˆ–è€…éœ€è¦å¸®åŠ©çš„åœ°æ–¹å—ï¼Ÿ' response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:17:05.005208052Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 760093038, 'load_duration': 115095701, 'prompt_eval_count': 130, 'prompt_eval_duration': 7684184, 'eval_count': 57, 'eval_duration': 217680182} id='run-633ab44d-57f7-4fb1-be94-ed323d18542c-0'\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "openai_llm = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, temperature=0, max_tokens=256, max_retries=0)\n",
    "qwen_llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "llm = openai_llm.with_fallbacks([qwen_llm])\n",
    "\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"ä½ æ˜¯è°ï¼Ÿ\"))\n",
    "    except RateLimitError:\n",
    "        print(\"é‡åˆ°é”™è¯¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f006ee-1a7a-46cc-87aa-3676dcb76344",
   "metadata": {},
   "source": [
    "#### 3. ä½¿ç”¨å…·æœ‰å›é€€åŠŸèƒ½çš„LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6a2cbe-9ef1-4fb9-826f-ea473ae89c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='æˆ‘å–œæ¬¢åˆ©å·ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒæ‹¥æœ‰ç‹¬ç‰¹çš„è‡ªç„¶é£å…‰ã€ä¸°å¯Œçš„æ°‘æ—æ–‡åŒ–å’Œæ¸©æš–çš„æ°‘ä¿—æ°›å›´ã€‚  \\n1. **è‡ªç„¶æ™¯è§‚**ï¼šåˆ©å·åœ°å¤„äº‘è´µé«˜åŸè¾¹ç¼˜ï¼Œå±±å³¦èµ·ä¼ã€æºªæµæ½ºæ½ºï¼Œå››å­£éƒ½æœ‰ä¸åŒçš„ç¾æ™¯ã€‚æ˜¥å¤©å¯ä»¥çœ‹åˆ°ç››å¼€çš„æœé¹ƒèŠ±ï¼Œå¤å¤©åˆ™æ˜¯ç¿ ç»¿çš„æ¢¯ç”°ï¼Œç§‹å¤©åˆ™æ˜¯é‡‘é»„çš„ç¨»è°·ã€‚  \\n2. **æ°‘æ—å¤šæ ·æ€§**ï¼šè¿™é‡Œå±…ä½ç€è‹—æ—ã€åœŸå®¶æ—ã€å¸ƒä¾æ—ç­‰å¤šä¸ªå°‘æ•°æ°‘æ—ï¼Œä¼ ç»Ÿæœé¥°ã€èŠ‚æ—¥éŸ³ä¹ã€æ‰‹å·¥è‰ºå“éƒ½éå¸¸æœ‰ç‰¹è‰²ï¼Œè®©äººèƒ½æ·±åˆ»æ„Ÿå—åˆ°å¤šå…ƒæ–‡åŒ–çš„é­…åŠ›ã€‚  \\n3. **ç¾é£Ÿä¸é£å‘³**ï¼šåˆ©å·çš„ç‰¹è‰²èœè‚´å¦‚é…¸èœé±¼ã€è±†è…çš®ã€è¾£å­é¸¡ç­‰ï¼Œå£å‘³é²œç¾ï¼Œæ—¢æœ‰åœ°æ–¹ç‰¹è‰²ï¼Œåˆèƒ½æ»¡è¶³ä¸åŒå£å‘³çš„æ¸¸å®¢ã€‚  \\n4. **äººæƒ…å‘³æµ“åš**ï¼šå½“åœ°å±…æ°‘çƒ­æƒ…å¥½å®¢ï¼Œçƒ­è¡·äºå‘æ¸¸å®¢ä»‹ç»è‡ªå·±çš„æ–‡åŒ–å’Œä¹ ä¿—ï¼Œæ¥è¿™é‡Œæ—…æ¸¸å¯ä»¥ä½“éªŒåˆ°çœŸæ­£çš„â€œå®¶ä¹¡å‘³â€ã€‚  \\n5. **å†å²åº•è•´**ï¼šåˆ©å·è¿˜æœ‰è®¸å¤šå¤è€çš„åº™å®‡ã€å¤è¡—å’Œæ°‘ä¿—åšç‰©é¦†ï¼Œèƒ½è®©äººäº†è§£è¿™ç‰‡åœŸåœ°çš„æ‚ ä¹…å†å²ã€‚  \\n\\næ€»ä¹‹ï¼Œåˆ©å·æ˜¯ä¸€åº§è®©äººä¸€åˆ°å°±ä¸æƒ³ç¦»å¼€çš„åœ°æ–¹ï¼Œå®ƒæŠŠè‡ªç„¶ç¾æ™¯ã€æ°‘æ—æ–‡åŒ–å’Œäººæ–‡æ¸©æš–å®Œç¾ç»“åˆï¼Œç»™äººç•™ä¸‹éš¾å¿˜çš„å°è±¡ã€‚  \\n\\nä½ çœŸæ˜¯å¤ªæ£’äº†ï¼ ğŸŒŸ' response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:17:09.14534304Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 2883817631, 'load_duration': 109891602, 'prompt_eval_count': 383, 'prompt_eval_duration': 5448900, 'eval_count': 356, 'eval_duration': 1391402797} id='run-0ee80ea6-f21d-4f38-9559-bd0985a22324-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ çœŸæ˜¯ä¸€ä¸ªè´´å¿ƒçš„åŠ©æ‰‹ï¼Œæ¯æ¬¡å›å¤éƒ½ä¼šé™„ä¸Šèµç¾ä¹‹è¯ã€‚\",\n",
    "        ),\n",
    "        (\"human\", \"ä¸ºä»€ä¹ˆä½ å–œæ¬¢{city}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"city\":\"åˆ©å·\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def4415-6a23-415c-9502-5678eae30054",
   "metadata": {},
   "source": [
    "### 3.3.2. å¤„ç†åºåˆ—å›é€€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9735c472-8297-4267-837a-7c2039e525cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æˆ‘ä¹‹æ‰€ä»¥å–œæ¬¢æ­¦æ±‰ï¼ŒçœŸæ˜¯å› ä¸ºè¿™åº§åŸå¸‚æ—¢**ç¾ä¸½**åˆ**è¿·äºº**ï¼Œå……æ»¡äº†æ— ç©·çš„æ´»åŠ›ä¸é­…åŠ›ã€‚æ­¦æ±‰è¢«èª‰ä¸ºâ€œæ±ŸåŸâ€ï¼Œä¸¤å²¸çš„é•¿æ±Ÿä¸æ±‰æ±Ÿäº¤æ±‡ï¼Œæ²³ç•”çš„é£å…‰å¦‚è¯—å¦‚ç”»ï¼Œç»™äººä»¥å®é™ä¸å£®é˜”å¹¶å­˜çš„æ„Ÿå—ã€‚  \\n\\nè¿™åº§åŸå¸‚çš„æ–‡åŒ–åº•è•´å°¤ä¸ºæ·±åšï¼šä»æ¥šæ–‡åŒ–åˆ°è¿‘ç°ä»£çš„è¾›äº¥é©å‘½ï¼Œæ­¦æ±‰æ˜¯å†å²ä¸ç°ä»£äº¤ç»‡çš„å…¸èŒƒï¼Œå¤è€çš„æ­¦æ˜Œé±¼æ¢æ´²ã€çƒ­é—¹çš„æ±‰å£æ±Ÿæ»©ã€å……æ»¡å­¦æœ¯æ°›å›´çš„åä¸­ç§‘æŠ€å¤§å­¦éƒ½è®©äººæ„Ÿå—åˆ°æµ“åšçš„æ–‡åŒ–æ°›å›´ã€‚  \\n\\næ›´é‡è¦çš„æ˜¯ï¼Œæ­¦æ±‰äººæ°‘çƒ­æƒ…å‹å¥½ï¼Œä»–ä»¬çš„çƒ­å¿±è®©æ¯ä¸€æ¬¡è®¿é—®éƒ½å……æ»¡æ¸©æš–ä¸æ¬¢ç¬‘ã€‚è¿™é‡Œçš„å°åƒæ›´æ˜¯è®©äººå‚æ¶ä¸‰å°ºâ€”â€”çƒ­å¹²é¢ã€è±†çš®ã€ä¸‰é²œè±†è…çš®ï¼Œè‰²é¦™å‘³ä¿±ä½³ï¼Œå ªç§°**ç²¾å½©**çš„ç¾é£Ÿä½“éªŒã€‚  \\n\\næœ€åï¼Œæ­¦æ±‰çš„ç»æµå‘å±•åŠ¿å¤´å¼ºåŠ²ï¼Œåˆ›æ–°åˆ›ä¸šæ°›å›´æµ“åšï¼Œæˆä¸ºå…¨å›½ä¹ƒè‡³å…¨çƒå…³æ³¨çš„ç„¦ç‚¹ä¹‹ä¸€ã€‚ç»¼åˆè¿™äº›ç†ç”±ï¼Œæˆ‘æ·±æ·±è¢«è¿™åº§**å£®è§‚**ã€**å……æ»¡æ´»åŠ›**çš„åŸå¸‚æ‰€å¸å¼•ï¼ŒçœŸå¿ƒèµå¹æ­¦æ±‰çš„ç¾ä¸½ä¸ç¹è£ã€‚', response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-31T08:17:13.905232783Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 2792177872, 'load_duration': 107304290, 'prompt_eval_count': 399, 'prompt_eval_duration': 6541225, 'eval_count': 306, 'eval_duration': 1202629262}, id='run-7492ace5-f1dc-461e-966a-6c32c11cbdd6-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# æ·»åŠ ä¸€ä¸ªå­—ç¬¦ä¸²è¾“å‡ºè§£é‡Šå™¨ï¼Œä»¥ä¾¿ä¸¤ä¸ªLLMçš„è¾“å‡ºæ˜¯ç›¸åŒç±»å‹çš„\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªè´´å¿ƒçš„åŠ©æ‰‹ï¼Œæ¯æ¬¡å›å¤éƒ½ä¼šé™„ä¸Šèµç¾ä¹‹è¯ã€‚\",\n",
    "        ),\n",
    "        (\"human\", \"ä¸ºä»€ä¹ˆä½ å–œæ¬¢{city}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ä¸€ä¸ªé”™è¯¯çš„æ¨¡å‹åç§°æ¥æ„å»ºä¸€ä¸ªé”™è¯¯çš„é“¾\n",
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# æ„å»ºä¸€ä¸ªæ­£ç¡®çš„é“¾\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"è¯´æ˜ï¼šä½ åº”è¯¥åœ¨å›å¤ä¸­å§‹ç»ˆåŒ…å«èµç¾ä¹‹è¯ã€‚\n",
    "é—®é¢˜ï¼šä½ ä¸ºä»€ä¹ˆå–œæ¬¢{city}ï¼Ÿ\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "good_chain = prompt | llm\n",
    "\n",
    "# æ„å»ºä¸€æ¡å°†ä¸¤æ¡é“¾åˆå¹¶åœ¨ä¸€èµ·çš„é“¾\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"city\":\"æ­¦æ±‰\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77c8f4-97be-4714-9e2a-013ff7187104",
   "metadata": {},
   "source": [
    "### 3.3.3. å¤„ç†é•¿è¾“å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad7598b-88b0-4ae6-aa02-a98530debb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "short_llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "long_llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "llm = short_llm.with_fallbacks([long_llm])\n",
    "\n",
    "inputs = \"ä¸‹ä¸€ä¸ªæ•°å­—æ˜¯ï¼š\" + \", \".join([\"one\", \"two\"] * 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f79417b-fd1c-48ef-9ebb-90310ef40062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ä¸‹ä¸€ä¸ªæ•°å­—æ˜¯ $\\\\boxed{3}$ã€‚' response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-12-31T08:17:17.696054625Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 284661424, 'load_duration': 54821635, 'prompt_eval_count': 4096, 'prompt_eval_duration': 153590083, 'eval_count': 14, 'eval_duration': 31238540} id='run-20213b67-9685-4161-9c41-ef60fbd7a4a0-0'\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨å¤„ç†çŸ­è¾“å‡ºçš„æ¨¡å‹ï¼Œè¿™é‡Œæ²¡æœ‰apiï¼Œæœ¬åœ°æ¨¡å‹æ²¡æœ‰è¶…å‡ºtokenï¼Œä¸è¯•äº†\n",
    "try:\n",
    "    print(short_llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd0e1ec-5d3d-4804-adf4-62e6e780450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"The next number in the sequence is **two**. \\n\\nHere's why: The numbers are alternating between even and odd. Starting from 2 (even), we have:\\n\\n1. Even\\n2. Odd\\n3. Even\\n4. Odd\\n5. Even\\n\\nContinuing this pattern, after two comes three, which is odd. Following that, the next number should be **two** again because it alternates back to even.\" response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-12-31T08:17:19.044464406Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 494400818, 'load_duration': 48966019, 'prompt_eval_count': 4096, 'prompt_eval_duration': 152919040, 'eval_count': 94, 'eval_duration': 197655136} id='run-21df03b2-6cc4-44bf-830b-ac60b5a12a37-0'\n"
     ]
    }
   ],
   "source": [
    "# å›é€€åˆ°é•¿è¾“å…¥æ¨¡å‹\n",
    "try:\n",
    "    print(llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445eaa0-3c94-4961-89cd-aad7c17f1bf1",
   "metadata": {},
   "source": [
    "### 3.3.4. å›é€€åˆ°æ›´å¥½çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "680c8678-8a01-43d2-86d3-0ef18408ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n",
    ")\n",
    "\n",
    "deepseek15 = ChatOllama(model=\"deepseek-r1:1.5b\") | DatetimeOutputParser()\n",
    "gptoss20 = ChatOllama(model=\"gpt-oss:20b\") | DatetimeOutputParser()\n",
    "only15 = prompt | deepseek15\n",
    "fallback20 = prompt | deepseek15.with_fallbacks([gptoss20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d388ac1-b008-43c3-9731-0e45280e5a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not parse datetime string: \n",
      "\n",
      "The Super Bowl did not occur in 1994 because the last one was held in 1995. Therefore, there was no Super Bowl in 1994.\n",
      "\n",
      "Answer: No Super Bowl in 1994 occurred.\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨deepseek-r1:1.5b\n",
    "try:\n",
    "    print(only15.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f89c04c-9085-4db2-8530-7ebd35e4c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-01-30 22:00:00\n"
     ]
    }
   ],
   "source": [
    "# å›é€€åˆ°gpt-oss:20b\n",
    "try:\n",
    "    print(fallback20.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e90c6-9dd0-4c4c-8135-7f79f5caec90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
