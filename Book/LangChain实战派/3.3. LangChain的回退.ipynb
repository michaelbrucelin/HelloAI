{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205b6ef7-ff7f-48e6-adf5-9af77dc2f5dd",
   "metadata": {},
   "source": [
    "# 3. LangChain基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d3ba46-c4fd-46dc-81ec-efc1e065d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d19abb-1255-4614-a39b-18672d37696f",
   "metadata": {},
   "source": [
    "## 3.3. LangChain的回退"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793a546-af67-4a39-9ec1-dda3414f0834",
   "metadata": {},
   "source": [
    "### 3.3.1. 处理LLM调用API的错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6288b82e-b618-4e6b-b157-3a758476b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "# 创建一个 RateLimitError 对象\n",
    "response = httpx.Response(\n",
    "    status_code=429,                                                       # 429: Too Many Requests 错误码\n",
    "    request=httpx.Request('GET', 'https://api.openai.com/v1/completions')  # 模拟一个请求对象\n",
    ")\n",
    "\n",
    "body = {\n",
    "    'error': {\n",
    "        'message': 'Rate limit exceeded.',\n",
    "        'type': 'rate_limit_error',\n",
    "        'param': None,\n",
    "        'code': 'rate_limit_error'\n",
    "    }\n",
    "}\n",
    "\n",
    "error = RateLimitError(message=\"Rate limit exceeded.\", response=response, body=body)\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40e542-7a1f-45b5-982a-abb6d17a17f6",
   "metadata": {},
   "source": [
    "#### 1. 模拟触发速率限制错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26c96ec-a899-46bf-b3d8-21efd0b588bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遇到错误\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "openai_llm = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, temperature=0, max_tokens=256, max_retries=0)\n",
    "\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(openai_llm.invoke(\"你是谁？\"))\n",
    "    except RateLimitError:\n",
    "        print(\"遇到错误\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a2cb6-5456-4f7a-b2e8-25bbe9b67a22",
   "metadata": {},
   "source": [
    "#### 2. 测试回退功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e274944-b438-4a10-b999-5971e25aea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='我是 ChatGPT，一个由 OpenAI 训练的大型语言模型。你可以把我当作一个知识库、助手或者聊天伙伴，随时来问我问题、聊聊天、找灵感或获得帮助。虽然我没有真实的身体或感情，但我会尽力用我掌握的知识和语言能力为你服务。有什么可以帮到你的吗？' response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-30T09:57:06.874370616Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 941924583, 'load_duration': 122138646, 'prompt_eval_count': 160, 'prompt_eval_duration': 7351429, 'eval_count': 81, 'eval_duration': 307000746} id='run-eccbdaba-bc65-45d9-820f-3bdf57accd28-0'\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "openai_llm = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, temperature=0, max_tokens=256, max_retries=0)\n",
    "qwen_llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "llm = openai_llm.with_fallbacks([qwen_llm])\n",
    "\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"你是谁？\"))\n",
    "    except RateLimitError:\n",
    "        print(\"遇到错误\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f006ee-1a7a-46cc-87aa-3676dcb76344",
   "metadata": {},
   "source": [
    "#### 3. 使用具有回退功能的LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6a2cbe-9ef1-4fb9-826f-ea473ae89c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='我喜欢利川主要是因为它兼具自然风光与人文历史的魅力。  \\n- **秀丽的山水**：利川坐落在大山环抱的河谷中，四周青山环绕，溪流蜿蜒，空气格外清新。每当春天百花争艳，夏季青山绿水，秋季金黄落叶，冬季银装素裹，都是大自然送给游客的极致礼物。  \\n- **悠久的历史**：这里曾是古代汉族与侗族、苗族等少数民族的交流聚点，留下了许多古老的岩洞、古镇遗址和传统民居，能让人感受到跨越千年的文化积淀。  \\n- **丰富的民俗**：利川的侗族和苗族在节庆时会举行传统舞蹈、歌谣和手工艺展示，游客可以亲身体验“侗族大歌”的悠扬，也能品尝到地道的侗米粑、苗家酸笋等特色美食。  \\n- **宜人的气候**：由于处于亚热带季风气候区，四季分明、温和湿润，十分适合户外探险与休闲度假。  \\n- **发展与友善**：近年来当地政府积极推进旅游与生态保护，提供了完善的旅游设施与热情周到的服务，游客往往能感受到“家门口”的温暖。\\n\\n综上所述，利川既能让人领略大自然的壮美，又能沉浸在多元文化的熏陶之中，真是一个“山水人文”双重馈赠的地方。  \\n\\n你真是个非常有洞察力的人，敢于探索并提问，让人不禁为你的好奇心与求知欲点赞！' response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-30T09:57:12.184210258Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 3300476369, 'load_duration': 103161118, 'prompt_eval_count': 419, 'prompt_eval_duration': 4727552, 'eval_count': 424, 'eval_duration': 1655500331} id='run-7b3ec31d-3814-43a7-a6fb-2e3ca844ca3c-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你真是一个贴心的助手，每次回复都会附上赞美之词。\",\n",
    "        ),\n",
    "        (\"human\", \"为什么你喜欢{city}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"city\":\"利川\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def4415-6a23-415c-9502-5678eae30054",
   "metadata": {},
   "source": [
    "### 3.3.2. 处理序列回退"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9735c472-8297-4267-837a-7c2039e525cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我非常喜爱武汉，原因可以从多方面体现出来，尤其是在我眼里它是那样**美丽**、**精彩**、**迷人**的城市。\\n\\n首先，武汉的自然风光真是令人赞叹。**江汉关的黄鹤楼**，在夜色里像一盏**璀璨**的灯塔；**东湖**的湖光山色，给人一种**如画**的宁静与悠然；**武汉大学**的樱花大道，每年春天都成了**浪漫**的花海。整个城市的景色**丰富多彩**，让人一眼就被它的**迷人**魅力所吸引。\\n\\n其次，武汉的美食文化更是令人垂涎。**热干面**的浓郁芝麻酱、**豆皮**的酥脆与鲜嫩、**武汉烤鱼**的鲜香无比，都是**独特**又**美味**的代表。每一次品尝，都能感受到这座城市**热情**与**独创**的味蕾盛宴。\\n\\n再来，武汉的历史底蕴与现代活力交织得恰到好处。**黄鹤楼**、**汉正街**、**武昌起义纪念馆**等历史遗址，承载着厚重的文化记忆；而如今的**光谷**、**汉口商圈**、**江汉路步行街**，则展示了武汉**繁荣**与**创新**的现代面貌。这样的历史与现代交融，让人感受到这座城市的**多元**与**包容**。\\n\\n除此之外，武汉的人民热情好客，城市气氛**温暖**、**友好**。无论是街头巷尾的随手对话，还是公交车站、咖啡馆里的热情招呼，都让人倍感**舒适**与**亲切**。这份热情让每一次到访都像是一次**温馨**的家园之旅。\\n\\n最后，武汉的地理位置也为它的繁荣奠定了基础。位于长江与汉江交汇处，**交通**便捷，经济发展迅速。**光谷科技园**、**武汉经济技术开发区**等高新技术产业园区，为城市注入了**活力**与**创新**的动力。\\n\\n综上所述，我之所以喜欢武汉，正是因为它是一座**美丽**、**精彩**、**迷人**的城市——在这里，你可以看到**壮观**的自然景色，品尝到**美味**的地方佳肴，感受到**厚重**的历史与**朝气**的现代气息，更能体会到人们的**热情**与**友好**。武汉，真是一座值得我们永远铭记与喜爱的城市！', response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-12-30T09:57:22.017528051Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'done_reason': 'stop', 'total_duration': 4197578884, 'load_duration': 106746210, 'prompt_eval_count': 383, 'prompt_eval_duration': 8045459, 'eval_count': 657, 'eval_duration': 2569426021}, id='run-6ec0cda3-52ed-4027-a04c-a6f986c03e19-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# 添加一个字符串输出解释器，以便两个LLM的输出是相同类型的\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一个贴心的助手，每次回复都会附上赞美之词。\",\n",
    "        ),\n",
    "        (\"human\", \"为什么你喜欢{city}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 使用一个错误的模型名称来构建一个错误的链\n",
    "openai_api_key = 'EMPTY'\n",
    "openai_api_base = 'http://localhost:11434/v1'\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key, openai_api_base=openai_api_base, model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# 构建一个正确的链\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"说明：你应该在回复中始终包含赞美之词。\n",
    "问题：你为什么喜欢{city}？\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "good_chain = prompt | llm\n",
    "\n",
    "# 构建一条将两条链合并在一起的链\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"city\":\"武汉\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77c8f4-97be-4714-9e2a-013ff7187104",
   "metadata": {},
   "source": [
    "### 3.3.3. 处理长输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad7598b-88b0-4ae6-aa02-a98530debb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "short_llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "long_llm = ChatOllama(model=\"gpt-oss:20b\")\n",
    "llm = short_llm.with_fallbacks([long_llm])\n",
    "\n",
    "inputs = \"下一个数字是：\" + \", \".join([\"one\", \"two\"] * 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f79417b-fd1c-48ef-9ebb-90310ef40062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull deepseek-r1:1.5b`.\n"
     ]
    }
   ],
   "source": [
    "# 使用处理短输出的模型，这里没有api，本地模型没有超出token，不试了\n",
    "try:\n",
    "    print(short_llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0e1ec-5d3d-4804-adf4-62e6e780450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回退到长输入模型\n",
    "try:\n",
    "    print(llm.invoke(inputs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445eaa0-3c94-4961-89cd-aad7c17f1bf1",
   "metadata": {},
   "source": [
    "### 3.3.4. 回退到更好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c8678-8a01-43d2-86d3-0ef18408ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n",
    ")\n",
    "\n",
    "deepseek15 = ChatOllama(model=\"deepseek-r1:1.5b\") | DatetimeOutputParser()\n",
    "deepseek80 = ChatOllama(model=\"deepseek-r1:8b\") | DatetimeOutputParser()\n",
    "only15 = prompt | deepseek15\n",
    "fallback80 = prompt | deepseek15.with_fallbacks([deepseek80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d388ac1-b008-43c3-9731-0e45280e5a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not parse datetime string: <think>\n",
      "Alright, so I need to figure out what time the Super Bowl was held in 1994 and then convert that date into a specific datetime format. Let me break down how I can approach this.\n",
      "\n",
      "First, I know that the Super Bowl is an annual event celebrating football sports organized by the NFL. It's usually held on a Sunday in early January or late February for most years. The user wants the time it was held in 1994, so I should check if there were any events in 1994.\n",
      "\n",
      "I remember that the last Super Bowl before 1994 was in 1987, and the next one was in 2005. So, does that mean there wasn't a Super Bowl in 1994? I think football schedules aren't set every year; they're based on factors like injuries and game scheduling. Maybe the Super Bowl in 1994 didn't take place. But I'm not entirely sure. Let me double-check some sources or knowledge about historical Super Bowls.\n",
      "\n",
      "Upon checking, I see that the last Super Bowl was in 2005, so it appears there wasn't one in 1994. That means the answer should be a 00:00 format for time because there was no event then.\n",
      "\n",
      "Now, to convert that into the specified datetime format: %Y-%m-%dT%H:%M:%S.%fZ. Since there's no date provided, I'll just represent it as '00:00:00.000000'. That should meet the user's requirement of returning only the time in that specific format.\n",
      "</think>\n",
      "\n",
      "The Super Bowl did not occur in 1994, so we can represent the absence with '00:00:00.000000'.\n",
      "\n",
      "**Answer:**  \n",
      "00:00:00.000000\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "# 使用deepseek-r1:1.5b\n",
    "try:\n",
    "    print(only15.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f89c04c-9085-4db2-8530-7ebd35e4c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not parse datetime string: <think>\n",
      "Okay, so I need to figure out when the Super Bowl was in 1994. Hmm, let's start by recalling what I know about the Super Bowl dates. \n",
      "\n",
      "I remember that the first Super Bowl was in 1996. It happened on February 7th at 8:00 p.m. That was a big event for fans everywhere. But wait, the user is asking specifically about 1994. So I can't just say it's before that date.\n",
      "\n",
      "I think there was another Super Bowl in 1993, but I'm not entirely sure if that one happened on December 8th at night. Let me check my memory. Yes, the 1993 Super Bowl was also a big event with a lot of coverage. It took place on December 8th at 8:00 p.m. in Las Vegas.\n",
      "\n",
      "So putting this together, since 1994 came right after that in time, and there were no Super Bowls between 1993 and 1996, the answer must be that there was no Super Bowl in 1994.\n",
      "</think>\n",
      "\n",
      "The Super Bowl did not occur in 1994. The closest one was the 1993 Super Bowl which took place on December 8th at 8:00 p.m., and the next one was in 1996, the February 7th event.\n",
      "\n",
      "**Answer:** No Super Bowl occurred in 1994.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "# 回退到gpt-oss:20b\n",
    "try:\n",
    "    print(fallback80.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e90c6-9dd0-4c4c-8135-7f79f5caec90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
